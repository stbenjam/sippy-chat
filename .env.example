# LLM Configuration
# For local Ollama (default)
LLM_ENDPOINT=http://localhost:11434/v1
MODEL_NAME=llama3.1:8b

# Sippy API Configuration (for future use)
SIPPY_API_URL=https://sippy.dptools.openshift.org

# Jira Configuration (for known incident tracking)
JIRA_URL=https://issues.redhat.com
