# LLM Configuration
# For local Ollama (default)
LLM_ENDPOINT=http://localhost:11434/v1
MODEL_NAME=llama3.1:8b

# For OpenAI (uncomment and set API key)
# LLM_ENDPOINT=https://api.openai.com/v1
# MODEL_NAME=gpt-3.5-turbo
# OPENAI_API_KEY=your_openai_api_key_here

# Sippy API Configuration (for future use)
SIPPY_API_URL=https://api.sippy.example.com

# Jira Configuration (for known incident tracking)
JIRA_URL=https://issues.redhat.com
# Optional: For authenticated access (if needed for private projects)
# JIRA_USERNAME=your_username
# JIRA_TOKEN=your_api_token
