# LLM Configuration
# For local Ollama (default)
# I have found that llama3.1:8b and granite3.3:8b give the best results,
# with a slight edge to granite.
LLM_ENDPOINT=http://localhost:11434/v1
MODEL_NAME=granite3.3:8b

# For OpenAI
# LLM_ENDPOINT=https://api.openai.com/v1
# MODEL_NAME=gpt-4
# OPENAI_API_KEY=your_openai_api_key_here

# For Google Gemini
# MODEL_NAME=gemini-1.5-pro
# GOOGLE_API_KEY=your_google_api_key_here

# Sippy API Configuration (for future use)
SIPPY_API_URL=https://sippy.dptools.openshift.org

# Jira Configuration (for known incident tracking)
JIRA_URL=https://issues.redhat.com
